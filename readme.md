# ğŸ•¸ï¸ Web Scraping Agent

This project is a **Node.js-based web scraping and site downloader tool** that allows you to crawl, process, and save static versions of websites locally.

It uses a modular structure with utilities for downloading assets, processing links, handling file paths, and managing the scraping engine.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ node_modules/          # Dependencies
â”œâ”€â”€ scraped-website/      # Downloaded website output
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ assetProcessor.js # Handles static assets (JS, CSS, images)
â”‚   â”œâ”€â”€ browserUtils.js   # Puppeteer/Playwright helpers (if used)
â”‚   â”œâ”€â”€ fileDownload.js   # File downloading utilities
â”‚   â”œâ”€â”€ index.js         # Entry point
â”‚   â”œâ”€â”€ LinkProcessor.js  # Processes links & rewrites them locally
â”‚   â”œâ”€â”€ pathUtils.js     # File path utilities
â”‚   â”œâ”€â”€ scrapingEngine.js # Core scraping logic
â”‚   â””â”€â”€ scrapingToll.js  # CLI/Tool runner
â”œâ”€â”€ .env.example         # Example environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ bun.lock            # Bun lockfile (if using Bun)
â”œâ”€â”€ package.json        # Dependencies & scripts
â””â”€â”€ package-lock.json   # NPM lockfile
```

## ğŸš€ Features

- Download and save a website locally
- Rewrite internal links to relative paths
- Save assets like JS, CSS, and images
- Handle Next.js/React/SPA builds by pre-rendering
- Simple modular code for easy extension

## âš™ï¸ Installation

Clone the repo and install dependencies:

```bash
git clone https://github.com/your-username/agent.git
cd agent
npm install
```

Or if you're using Bun:

```bash
bun install
```

## â–¶ï¸ Usage

Run the scraper with:

```bash
node src/index.js
```

If you're using Bun:

```bash
bun run src/index.js
```

Scraped websites will be saved inside the `scraped-website/` folder.

## ğŸ”§ Configuration

You can configure scraping settings inside `.env` (copy from `.env.example`):

```env
BASE_URL=https://example.com
OUTPUT_DIR=./scraped-website
```

## ğŸ“œ Scripts

Common scripts you can add to `package.json`:

```json
{
  "scripts": {
    "start": "node src/index.js",
    "scrape": "node src/scrapingToll.js"
  }
}
```

Run with:

```bash
npm run scrape
```

## ğŸ“Œ Notes

- Works best on websites with static builds (e.g., Next.js, Gatsby, static exports)
- For fully dynamic SPAs, Puppeteer/Playwright integration may be required
- Make sure to respect websites' robots.txt and terms of service

## ğŸ“ License

MIT License Â© 2025 Your Name
